{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BeautifulSoup is a web scraping library. Given the source of a web page, soup :\n",
    "\n",
    "- Organizes the page as a dictionary keyed by html tags\n",
    "- Can search the page using these tags and regular expressions\n",
    "- Simplifies the process of tag based search on a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request as ul #url.request lib for handling the url\n",
    "from bs4 import BeautifulSoup #bs for parsing the page\n",
    "\n",
    "url = \"http://www.slate.com\"\n",
    "\n",
    "#Do stuff necessary to get the page text into a string\n",
    "url_response=ul.urlopen(url,timeout=5)\n",
    "\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(url_response) #Soup stores the data in a structured way to make retrieval easy\n",
    "#Soup also automatically decodes the page correctly (most of the time!)\n",
    "\n",
    "print(soup.prettify()) #Prints page contents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Soup stores the page as a dictionary with tags for retrieval. \n",
    "#For example, the first div tag on the page is:\n",
    "soup.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Or, to get the first link out of the page\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To find all links on the page\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links=soup.find_all('a')\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Can we get all the links from finance.google.com using bs4?\n",
    "#Try it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can search for specific things in the list returned by find_all\n",
    "for link in soup.find_all('a'): #pick each link in turn\n",
    "    h_link=link.get('href') #Get the href property of the link\n",
    "    if h_link and 'articles' in h_link: #See if the word 'articles' is in the link\n",
    "        print(h_link)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Unfortunately, these links don't contain blogs, podcasts, videos etc. All of which are potential articles\n",
    "So let's take a look and see if we can get some more information from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Passing True to find_all gets all the tags in the document\n",
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#That's a long list. Let's see if we can get unique tag names from the document\n",
    "#Sets contain only unique elements so let's use a set\n",
    "#We can get the set of tags (without duplicates) as follows\n",
    "y=set()\n",
    "for tag in soup.find_all(True):\n",
    "    #print(tag.name)\n",
    "    y.add(tag.name) #add adds an item to a set - if it is not already in the set\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hmm. There is a tag called 'article'. Wonder what that is about?\n",
    "soup.article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Slate apparently uses the article tag to identify content.\n",
    "#Wonder if there are more. If not, then this will contain all the material we need\n",
    "len(soup.find_all('article'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It is the only one. Let's work with it\n",
    "article_soup=soup.article #soup was our original document. article_soup is an extract from that document\n",
    "print(article_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#So let's see if we can find articles with a particular name in the link\n",
    "search_name = 'trump'\n",
    "found_articles = list()\n",
    "for link_tag in article_soup.find_all('a'):\n",
    "    link = link_tag.get('href')\n",
    "    if link and search_name in link.lower(): #Because some a tags may return empty href elements. in won't work on those\n",
    "        found_articles.append(link)\n",
    "import pprint\n",
    "pprint.pprint(found_articles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's practice\n",
    "From the Google finance news page\n",
    "get all links\n",
    "print links that relate to global or international news (i.e., the word global or international is in the link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_2='http://www.google.com/finance/market_news'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#has_attr tells whether a tag has a particular attribute or not\n",
    "for link in article_soup.find_all('a'):\n",
    "    if link.has_attr('class'):\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Looks like there is extraneous stuff with attribute class. Let's focus only on the ones with\n",
    "class=\"primary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To check if an attribute has a certain value, use an equal sign for the attribute\n",
    "article_soup.find_all('a',class='primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unfortunately, class is a reserved word (it means something) in python\n",
    "#So, put an _ after class and it will work!\n",
    "article_soup.find_all('a',class_='primary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we can get exactly what we want\n",
    "for link in article_soup.find_all('a',class_='primary'):\n",
    "    h_link=link.get('href')\n",
    "    if h_link in h_link:\n",
    "        print(h_link)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can now functionalize the code so that given a word, we get a list of all the articles that have that\n",
    "#word in the url\n",
    "\n",
    "def find_links(search_term,url):\n",
    "    import urllib.request as ur\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup_stuff = BeautifulSoup(ur.urlopen(url))\n",
    "    found_links=list()\n",
    "    for link in soup_stuff.find_all('a'):\n",
    "        try:\n",
    "            \n",
    "            if link and search_term in link.get('href') and not link.get('href') in found_links:\n",
    "                found_links.append(link.get('href'))\n",
    "        except:\n",
    "            continue\n",
    "    return found_links\n",
    "find_links('trump','http://www.slate.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Another useful thing you can do with soup\n",
    "#Limit the number of responses. Useful if you need to control the search\n",
    "#Especially important when reading a large page and you only want a sampling\n",
    "article_soup.find_all('a',class_='primary',limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's see if we can get all links to main news articles from google finance news\n",
    "On examining the source, we'll see that main news articles are enclosed by the tags:\n",
    "    <span class='name'> .... </span>\n",
    "So, see if you can extract the links from within these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do it here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sending queries to the server:\n",
    "    Web servers typically act as a front end for a database\n",
    "    You send a request to the server, usually by entering data on a form on the browser\n",
    "    The server sends a request to a database\n",
    "    The database sends the query result to the server\n",
    "    The server sends a response to the client in the form of a html page\n",
    "    The browser at the client's end renders the html page\n",
    "\n",
    " Depending on how the form sends data, the query variables may be included in the url returned by the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functionalizing the search so that we can search any url for a search term\n",
    "\n",
    "def find_titles(search_term,url):\n",
    "    import urllib.request as ur\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup_stuff = BeautifulSoup(ur.urlopen(url))\n",
    "    found_links=list()\n",
    "    for link in soup_stuff.find_all('a'):\n",
    "        try:\n",
    "            if link and search_term in link.get_text().lower() and not link.get_text() in found_links:\n",
    "                found_links.append(link)\n",
    "        except:\n",
    "            continue\n",
    "    return found_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "find_titles('clinton','http://news.yahoo.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
